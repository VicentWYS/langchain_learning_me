# ğŸ“˜ LangChain 1.0 åŸºç¡€çŸ¥è¯†æ€»ç»“ â€”â€” ç¬¬ä¸€ä¸ª LLM è°ƒç”¨

> æœ¬æ–‡å¯¹åº”å­¦ä¹ ä»£ç ï¼šLangChain 1.0 ç¬¬ä¸€æ¬¡ LLM è°ƒç”¨å®Œæ•´ç¤ºä¾‹
> ç›®æ ‡ï¼šç†è§£ **LangChain å¦‚ä½•ç»Ÿä¸€ç®¡ç†ä¸åŒå‚å•†æ¨¡å‹çš„è°ƒç”¨**

---

## ğŸ§  ä¸€å¥è¯ç†è§£ LangChain 1.0

> **LangChain 1.0 ä¸å†å…³å¿ƒâ€œä½ æ˜¯ä»€ä¹ˆæ¨¡å‹â€ï¼Œåªå…³å¿ƒâ€œä½ æ˜¯å¦å…¼å®¹ OpenAI åè®®â€ã€‚**

æ‰€æœ‰æ¨¡å‹ï¼ˆQwen / Groq / OpenAI / Claudeï¼‰éƒ½é€šè¿‡åŒä¸€ä¸ªå…¥å£ï¼š

```python
init_chat_model()
```

---

## ğŸ”‘ ç¬¬ä¸€æ­¥ï¼šç¯å¢ƒå˜é‡ä¸é‰´æƒï¼ˆç”Ÿäº§çº§å¿…åšï¼‰

```python
load_dotenv()
QWEN_API_KEY = os.getenv("QWEN_API_KEY")
QWEN_BASE_URL = os.getenv("QWEN_BASE_URL")
```

### ä¸ºä»€ä¹ˆå¿…é¡»æå‰æ ¡éªŒï¼Ÿ

é˜²æ­¢ä¸‰ç±»**æéš¾æ’æŸ¥çš„é”™è¯¯**ï¼š

| é”™è¯¯æ¥æº        | ç°è±¡             | æ’æŸ¥éš¾åº¦ |
| ----------- | -------------- | ---- |
| .env æœªåŠ è½½    | api_key ä¸º None | å¾ˆé«˜   |
| å¿˜è®°æ›¿æ¢æ¨¡æ¿ key  | é‰´æƒå¤±è´¥           | å¾ˆé«˜   |
| base_url é…é”™ | æ¨¡å‹åˆå§‹åŒ–æŠ¥é”™        | å¾ˆé«˜   |

âœ… **æœ€ä½³å®è·µï¼šåœ¨æ¨¡å‹åˆå§‹åŒ–å‰å°± raise**

---

## ğŸ­ ç¬¬äºŒæ­¥ï¼šLangChain ç»Ÿä¸€æ¨¡å‹å·¥å‚

```python
model = init_chat_model(
    model="qwen-plus",
    model_provider="openai",
    api_key=QWEN_API_KEY,
    base_url=QWEN_BASE_URL,
    temperature=0.8,
)
```

### ğŸš¨ å…³é”®è®¤çŸ¥ï¼ˆéå¸¸é‡è¦ï¼‰

| è®¤çŸ¥                        | å«ä¹‰                    |
| ------------------------- | --------------------- |
| Qwen ä¸æ˜¯ OpenAI æ¨¡å‹         | ä½†å®ƒ**ä¼ªè£…æˆ OpenAI åè®®**   |
| LangChain ä¸çœ‹æ¨¡å‹å‚å•†          | åªçœ‹åè®®                  |
| `model_provider="openai"` | è¡¨ç¤ºèµ° OpenAI SDK åè®®     |
| `base_url`                | æŒ‡å‘ Qwen çš„ OpenAI å…¼å®¹ç«¯ç‚¹ |

> **LangChain 1.x çš„æ ¸å¿ƒè®¾è®¡å“²å­¦ï¼šåè®®æŠ½è±¡ï¼Œè€Œä¸æ˜¯å‚å•†æŠ½è±¡**

---

## ğŸš€ ç¤ºä¾‹1ï¼šæœ€ç®€å•çš„è°ƒç”¨æ–¹å¼

```python
response = model.invoke("ä½ å¥½ï¼è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½")
```

### ç‰¹ç‚¹

* ç›´æ¥ä¼ å­—ç¬¦ä¸²
* é€‚åˆå•è½®å¯¹è¯
* æœ€ç®€å•çš„ LLM è°ƒç”¨å½¢å¼

---

## ğŸ’¬ ç¤ºä¾‹2ï¼šä½¿ç”¨ Message æ„å»ºå¯¹è¯ï¼ˆé‡è¦ï¼‰

```python
messages = [
    SystemMessage(content="ä½ æ˜¯ Python åŠ©æ‰‹"),
    HumanMessage(content="ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ"),
]
response = model.invoke(messages)
```

### ä¸‰ç§æ¶ˆæ¯ç±»å‹

| ç±»å‹            | ä½œç”¨           |
| ------------- | ------------ |
| SystemMessage | è®¾å®š AI äººè®¾     |
| HumanMessage  | ç”¨æˆ·è¾“å…¥         |
| AIMessage     | AI å›å¤ï¼ˆå¯åŠ å…¥å†å²ï¼‰ |

### å¤šè½®å¯¹è¯æœ¬è´¨

```python
messages.append(response)
messages.append(HumanMessage(content="è¯·ç»™ç¤ºä¾‹"))
```

> **å¯¹è¯è®°å¿† = æ‰‹åŠ¨ç»´æŠ¤ messages åˆ—è¡¨**

---

## ğŸ§¾ ç¤ºä¾‹3ï¼šæ¨èçš„å­—å…¸æ¶ˆæ¯æ ¼å¼ï¼ˆğŸ”¥æ¨èï¼‰

```python
messages = [
    {"role": "system", "content": "..."},
    {"role": "user", "content": "..."},
]
```

### ä¸ºä»€ä¹ˆæ¨èï¼Ÿ

* ä¸ OpenAI API æ ¼å¼å®Œå…¨ä¸€è‡´
* æ›´è½»é‡
* æ›´é€‚åˆå·¥ç¨‹åŒ–
* æ–¹ä¾¿ä¸ Agent / LangGraph å¯¹æ¥

---

## ğŸ› ç¤ºä¾‹4ï¼šæ¨¡å‹å‚æ•°å¯¹è¾“å‡ºçš„å½±å“

| å‚æ•°               | ä½œç”¨     | åœºæ™¯        |
| ---------------- | ------ | --------- |
| temperature=0.0  | æåº¦ç¨³å®š   | æ•°æ®æŠ½å– / åˆ¤æ–­ |
| temperature=1.0  | å¹³è¡¡     | æ—¥å¸¸å¯¹è¯      |
| temperature=1.5+ | é«˜åˆ›é€ æ€§   | å†™ä½œ / åˆ›æ„   |
| max_tokens       | é™åˆ¶è¾“å‡ºé•¿åº¦ | é˜²æ­¢åºŸè¯      |

> **ä½ åœ¨åš Prompt å·¥ç¨‹æ—¶ï¼Œè¿™ä¸€èŠ‚éå¸¸å…³é”®**

---

## ğŸ“¦ ç¤ºä¾‹5ï¼šç†è§£ invoke çš„è¿”å›å€¼ï¼ˆæé‡è¦ï¼‰

```python
response = model.invoke(...)
```

è¿”å›çš„æ˜¯ï¼š

```python
AIMessage
```

### å…³é”®å­—æ®µ

| å­—æ®µ                         | å«ä¹‰           |
| -------------------------- | ------------ |
| response.content           | æ¨¡å‹æ–‡æœ¬         |
| response.response_metadata | token / æ¨¡å‹ä¿¡æ¯ |
| response.id                | æ¶ˆæ¯ID         |
| response.additional_kwargs | æ‰©å±•å‚æ•°         |

### Token ç»Ÿè®¡ï¼ˆåšæˆæœ¬ä¼˜åŒ–å¿…çœ‹ï¼‰

```python
usage = response.response_metadata["token_usage"]
```

---

## ğŸ›¡ ç¤ºä¾‹6ï¼šç”Ÿäº§çº§é”™è¯¯å¤„ç†

å¿…é¡»æ•è·ï¼š

```python
try:
    model.invoke(...)
except ValueError:
except ConnectionError:
except Exception:
```

> çœŸå®é¡¹ç›®ä¸­ï¼š**80% çš„é—®é¢˜æ¥è‡ªé‰´æƒå’Œç½‘ç»œ**

---

## ğŸ”„ ç¤ºä¾‹7ï¼šä¸€é”®åˆ‡æ¢ä¸åŒæ¨¡å‹ï¼ˆLangChain æœ€å¤§ä¼˜åŠ¿ï¼‰

```python
models_to_test = [
    "groq:llama-3.3-70b-versatile",
    "groq:mixtral-8x7b-32768",
]
```

LangChain çš„çœŸæ­£å¨åŠ›ï¼š

> **åªæ”¹æ¨¡å‹åï¼Œä¸æ”¹ä»£ç **

---

## ğŸ§  æœ¬æ–‡ä»¶çœŸæ­£æ•™ä¼šä½ çš„ 8 ä¸ªæ ¸å¿ƒè®¤çŸ¥

1. LangChain é€šè¿‡ `init_chat_model` ç»Ÿä¸€æ‰€æœ‰æ¨¡å‹
2. LangChain åªå…³å¿ƒåè®®ï¼Œä¸å…³å¿ƒå‚å•†
3. Qwen å¿…é¡»èµ° OpenAI-compatible endpoint
4. `invoke` å¯ä»¥æ¥æ”¶å­—ç¬¦ä¸² / Message / dict
5. å¯¹è¯è®°å¿†çš„æœ¬è´¨æ˜¯ç»´æŠ¤ messages
6. æ¨èä½¿ç”¨ dict message æ ¼å¼
7. temperature å†³å®šè¾“å‡ºé£æ ¼
8. `invoke` è¿”å›çš„æ˜¯ AIMessageï¼Œä¸æ˜¯å­—ç¬¦ä¸²

---

## ğŸ—º LangChain LLM è°ƒç”¨å®Œæ•´æµç¨‹å›¾

```
.env â†’ æ ¡éªŒ â†’ init_chat_model â†’ invoke â†’ AIMessage â†’ è¯»å– content
```

å¤šè½®å¯¹è¯ï¼š

```
messages åˆ—è¡¨ â† AIMessage è¿½åŠ  â† å†æ¬¡ invoke
```

---

## âœ… æœ€ä½³å®è·µæ¨¡æ¿ï¼ˆå¯ç›´æ¥å¤ç”¨ï¼‰

```python
model = init_chat_model(...)

messages = [
    {"role": "system", "content": "ä½ æ˜¯..."},
    {"role": "user", "content": "..."}
]

response = model.invoke(messages)
print(response.content)
```

---

## ğŸ¯ ä½ éœ€è¦è®°ä½çš„ä¸æ˜¯ä»£ç ï¼Œè€Œæ˜¯è¿™å¥è¯

> **LangChain 1.0 = OpenAI åè®® + init_chat_model + invoke + messages**
